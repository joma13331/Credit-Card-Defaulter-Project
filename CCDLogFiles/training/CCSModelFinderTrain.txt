INFO 06/03/2022 08:04:40 PM TRAINING: Search for best model started
INFO 06/03/2022 08:04:40 PM TRAINING: Search for best logistic regressor model started
INFO 06/03/2022 08:04:40 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver', 'l1_ratio']))  of Logistic Regressor
INFO 06/03/2022 08:05:51 PM TRAINING: Search for best model started
INFO 06/03/2022 08:05:51 PM TRAINING: Search for best logistic regressor model started
INFO 06/03/2022 08:05:51 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver', 'l1_ratio']))  of Logistic Regressor
INFO 06/03/2022 08:08:27 PM TRAINING: Search for best model started
INFO 06/03/2022 08:08:27 PM TRAINING: Search for best logistic regressor model started
INFO 06/03/2022 08:08:27 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver', 'l1_ratio']))  of Logistic Regressor
ERROR 06/03/2022 08:08:28 PM TRAINING: There was a problem while fitting Logistic Regressor: [Errno 22] Invalid argument
ERROR 06/03/2022 08:08:28 PM TRAINING: There was a problem while obtaining best model : [Errno 22] Invalid argument
INFO 06/03/2022 08:09:14 PM TRAINING: Search for best model started
INFO 06/03/2022 08:09:14 PM TRAINING: Search for best logistic regressor model started
INFO 06/03/2022 08:09:14 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
ERROR 06/03/2022 08:09:33 PM TRAINING: There was a problem while fitting Logistic Regressor: 'l1_ratio'
ERROR 06/03/2022 08:09:33 PM TRAINING: There was a problem while obtaining best model : 'l1_ratio'
INFO 06/03/2022 08:11:02 PM TRAINING: Search for best model started
INFO 06/03/2022 08:11:02 PM TRAINING: Search for best logistic regressor model started
INFO 06/03/2022 08:11:02 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/03/2022 08:11:21 PM TRAINING: The optimum parameters of Logistic Regressor are C=1, penalty=l2,solver=newton-cg  with the f1 score of 0.001568627450980392
INFO 06/03/2022 08:11:21 PM TRAINING: Best Logistic  Regressor trained
ERROR 06/03/2022 08:11:21 PM TRAINING: There was a problem while obtaining best model : f1_score() got an unexpected keyword argument 'y_score'
INFO 06/03/2022 08:12:35 PM TRAINING: Search for best model started
INFO 06/03/2022 08:12:35 PM TRAINING: Search for best logistic regressor model started
INFO 06/03/2022 08:12:35 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/03/2022 08:12:53 PM TRAINING: The optimum parameters of Logistic Regressor are C=1, penalty=l2,solver=newton-cg  with the f1 score of 0.001568627450980392
INFO 06/03/2022 08:12:53 PM TRAINING: Best Logistic  Regressor trained
INFO 06/03/2022 08:12:53 PM TRAINING: Search for best ridge model ended
INFO 06/03/2022 08:12:53 PM TRAINING: Search for best svc model started
INFO 06/03/2022 08:12:53 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/03/2022 08:23:40 PM TRAINING: The optimum parameters of SVC are kernel=sigmoid, gamma=auto, C=10, degree =2 with the f1 score of 0.20465551286843775
INFO 06/03/2022 08:23:42 PM TRAINING: Best SVC trained
INFO 06/03/2022 08:23:42 PM TRAINING: Search for best svc model ended
INFO 06/03/2022 08:23:42 PM TRAINING: Search for best random forest classifier model started
INFO 06/03/2022 08:23:42 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/03/2022 08:43:36 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=30, criterion=entropy, min_samples_split=2, max_features =auto, ccp_alpha=0.0 with the adjusted R2 score of 0.1658065455349223
INFO 06/03/2022 08:43:37 PM TRAINING: Best random forest classifier trained
INFO 06/03/2022 08:43:37 PM TRAINING: Search for best random forest classifier model ended
INFO 06/03/2022 08:43:37 PM TRAINING: Search for best xgb classifier model started
INFO 06/03/2022 08:43:37 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/03/2022 09:40:36 PM TRAINING: The optimum parameters of xgb-classifier are learning_rate=1, max_depth=3, colsample_bytree=0.4, n_estimators =3000 with the adjusted R2 score of 0.23847222470610635
INFO 06/03/2022 09:40:46 PM TRAINING: Best xgb classifier trained
INFO 06/03/2022 09:40:46 PM TRAINING: Search for best xgb classifier model ended
INFO 06/03/2022 09:40:46 PM TRAINING: The best model is random forest classifier
INFO 06/03/2022 09:40:46 PM TRAINING: Search for best model started
INFO 06/03/2022 09:40:46 PM TRAINING: Search for best model started
INFO 06/03/2022 09:40:46 PM TRAINING: Search for best logistic regressor model started
INFO 06/03/2022 09:40:46 PM TRAINING: Search for best logistic regressor model started
INFO 06/03/2022 09:40:46 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/03/2022 09:40:46 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/03/2022 09:40:55 PM TRAINING: The optimum parameters of Logistic Regressor are C=3, penalty=l2,solver=newton-cg  with the f1 score of 0.5171360625442198
INFO 06/03/2022 09:40:55 PM TRAINING: The optimum parameters of Logistic Regressor are C=3, penalty=l2,solver=newton-cg  with the f1 score of 0.5171360625442198
INFO 06/03/2022 09:40:55 PM TRAINING: Best Logistic  Regressor trained
INFO 06/03/2022 09:40:55 PM TRAINING: Best Logistic  Regressor trained
INFO 06/03/2022 09:40:55 PM TRAINING: Search for best ridge model ended
INFO 06/03/2022 09:40:55 PM TRAINING: Search for best ridge model ended
INFO 06/03/2022 09:40:55 PM TRAINING: Search for best svc model started
INFO 06/03/2022 09:40:55 PM TRAINING: Search for best svc model started
INFO 06/03/2022 09:40:55 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/03/2022 09:40:55 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/03/2022 10:14:22 PM TRAINING: The optimum parameters of SVC are kernel=linear, gamma=scale, C=0.1, degree =2 with the f1 score of 0.5969411932188955
INFO 06/03/2022 10:14:22 PM TRAINING: The optimum parameters of SVC are kernel=linear, gamma=scale, C=0.1, degree =2 with the f1 score of 0.5969411932188955
INFO 06/03/2022 10:14:25 PM TRAINING: Best SVC trained
INFO 06/03/2022 10:14:25 PM TRAINING: Best SVC trained
INFO 06/03/2022 10:14:26 PM TRAINING: Search for best svc model ended
INFO 06/03/2022 10:14:26 PM TRAINING: Search for best svc model ended
INFO 06/03/2022 10:14:26 PM TRAINING: Search for best random forest classifier model started
INFO 06/03/2022 10:14:26 PM TRAINING: Search for best random forest classifier model started
INFO 06/03/2022 10:14:26 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/03/2022 10:14:26 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/03/2022 10:50:53 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=50, criterion=entropy, min_samples_split=6, max_features =log2, ccp_alpha=0.001 with the adjusted R2 score of 0.5631162219875744
INFO 06/03/2022 10:50:53 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=50, criterion=entropy, min_samples_split=6, max_features =log2, ccp_alpha=0.001 with the adjusted R2 score of 0.5631162219875744
INFO 06/03/2022 10:50:54 PM TRAINING: Best random forest classifier trained
INFO 06/03/2022 10:50:54 PM TRAINING: Best random forest classifier trained
INFO 06/03/2022 10:50:54 PM TRAINING: Search for best random forest classifier model ended
INFO 06/03/2022 10:50:54 PM TRAINING: Search for best random forest classifier model ended
INFO 06/03/2022 10:50:54 PM TRAINING: Search for best xgb classifier model started
INFO 06/03/2022 10:50:54 PM TRAINING: Search for best xgb classifier model started
INFO 06/03/2022 10:50:54 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/03/2022 10:50:54 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/04/2022 12:30:53 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.3, max_depth=3, colsample_bytree=0.5, n_estimators =30 with the adjusted R2 score of 0.5638597329103255
INFO 06/04/2022 12:30:53 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.3, max_depth=3, colsample_bytree=0.5, n_estimators =30 with the adjusted R2 score of 0.5638597329103255
INFO 06/04/2022 12:30:53 AM TRAINING: Best xgb classifier trained
INFO 06/04/2022 12:30:53 AM TRAINING: Best xgb classifier trained
INFO 06/04/2022 12:30:53 AM TRAINING: Search for best xgb classifier model ended
INFO 06/04/2022 12:30:53 AM TRAINING: Search for best xgb classifier model ended
INFO 06/04/2022 12:30:53 AM TRAINING: The best model is svc
INFO 06/04/2022 12:30:53 AM TRAINING: The best model is svc
INFO 06/04/2022 12:30:53 AM TRAINING: Search for best model started
INFO 06/04/2022 12:30:53 AM TRAINING: Search for best model started
INFO 06/04/2022 12:30:53 AM TRAINING: Search for best model started
INFO 06/04/2022 12:30:53 AM TRAINING: Search for best logistic regressor model started
INFO 06/04/2022 12:30:53 AM TRAINING: Search for best logistic regressor model started
INFO 06/04/2022 12:30:53 AM TRAINING: Search for best logistic regressor model started
INFO 06/04/2022 12:30:53 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/04/2022 12:30:53 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/04/2022 12:30:53 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/04/2022 12:31:06 AM TRAINING: The optimum parameters of Logistic Regressor are C=0.3, penalty=l1,solver=saga  with the f1 score of 0.5443714855844652
INFO 06/04/2022 12:31:06 AM TRAINING: The optimum parameters of Logistic Regressor are C=0.3, penalty=l1,solver=saga  with the f1 score of 0.5443714855844652
INFO 06/04/2022 12:31:06 AM TRAINING: The optimum parameters of Logistic Regressor are C=0.3, penalty=l1,solver=saga  with the f1 score of 0.5443714855844652
INFO 06/04/2022 12:31:06 AM TRAINING: Best Logistic  Regressor trained
INFO 06/04/2022 12:31:06 AM TRAINING: Best Logistic  Regressor trained
INFO 06/04/2022 12:31:06 AM TRAINING: Best Logistic  Regressor trained
INFO 06/04/2022 12:31:06 AM TRAINING: Search for best ridge model ended
INFO 06/04/2022 12:31:06 AM TRAINING: Search for best ridge model ended
INFO 06/04/2022 12:31:06 AM TRAINING: Search for best ridge model ended
INFO 06/04/2022 12:31:06 AM TRAINING: Search for best svc model started
INFO 06/04/2022 12:31:06 AM TRAINING: Search for best svc model started
INFO 06/04/2022 12:31:06 AM TRAINING: Search for best svc model started
INFO 06/04/2022 12:31:06 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/04/2022 12:31:06 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/04/2022 12:31:06 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/04/2022 12:41:25 AM TRAINING: The optimum parameters of SVC are kernel=linear, gamma=scale, C=0.01, degree =2 with the f1 score of 0.58241233445468
INFO 06/04/2022 12:41:25 AM TRAINING: The optimum parameters of SVC are kernel=linear, gamma=scale, C=0.01, degree =2 with the f1 score of 0.58241233445468
INFO 06/04/2022 12:41:25 AM TRAINING: The optimum parameters of SVC are kernel=linear, gamma=scale, C=0.01, degree =2 with the f1 score of 0.58241233445468
INFO 06/04/2022 12:41:25 AM TRAINING: Best SVC trained
INFO 06/04/2022 12:41:25 AM TRAINING: Best SVC trained
INFO 06/04/2022 12:41:25 AM TRAINING: Best SVC trained
INFO 06/04/2022 12:41:25 AM TRAINING: Search for best svc model ended
INFO 06/04/2022 12:41:25 AM TRAINING: Search for best svc model ended
INFO 06/04/2022 12:41:25 AM TRAINING: Search for best svc model ended
INFO 06/04/2022 12:41:25 AM TRAINING: Search for best random forest classifier model started
INFO 06/04/2022 12:41:25 AM TRAINING: Search for best random forest classifier model started
INFO 06/04/2022 12:41:25 AM TRAINING: Search for best random forest classifier model started
INFO 06/04/2022 12:41:25 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/04/2022 12:41:25 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/04/2022 12:41:25 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/04/2022 12:50:31 AM TRAINING: The optimum parameters of random forrest classifier are n_estimators=300, criterion=gini, min_samples_split=6, max_features =log2, ccp_alpha=0.002 with the adjusted R2 score of 0.5763985973125072
INFO 06/04/2022 12:50:31 AM TRAINING: The optimum parameters of random forrest classifier are n_estimators=300, criterion=gini, min_samples_split=6, max_features =log2, ccp_alpha=0.002 with the adjusted R2 score of 0.5763985973125072
INFO 06/04/2022 12:50:31 AM TRAINING: The optimum parameters of random forrest classifier are n_estimators=300, criterion=gini, min_samples_split=6, max_features =log2, ccp_alpha=0.002 with the adjusted R2 score of 0.5763985973125072
INFO 06/04/2022 12:50:32 AM TRAINING: Best random forest classifier trained
INFO 06/04/2022 12:50:32 AM TRAINING: Best random forest classifier trained
INFO 06/04/2022 12:50:32 AM TRAINING: Best random forest classifier trained
INFO 06/04/2022 12:50:32 AM TRAINING: Search for best random forest classifier model ended
INFO 06/04/2022 12:50:32 AM TRAINING: Search for best random forest classifier model ended
INFO 06/04/2022 12:50:32 AM TRAINING: Search for best random forest classifier model ended
INFO 06/04/2022 12:50:32 AM TRAINING: Search for best xgb classifier model started
INFO 06/04/2022 12:50:32 AM TRAINING: Search for best xgb classifier model started
INFO 06/04/2022 12:50:32 AM TRAINING: Search for best xgb classifier model started
INFO 06/04/2022 12:50:32 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/04/2022 12:50:32 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/04/2022 12:50:32 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/04/2022 01:09:47 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=3, colsample_bytree=0.9, n_estimators =30 with the adjusted R2 score of 0.5585401886401213
INFO 06/04/2022 01:09:47 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=3, colsample_bytree=0.9, n_estimators =30 with the adjusted R2 score of 0.5585401886401213
INFO 06/04/2022 01:09:47 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=3, colsample_bytree=0.9, n_estimators =30 with the adjusted R2 score of 0.5585401886401213
INFO 06/04/2022 01:09:47 AM TRAINING: Best xgb classifier trained
INFO 06/04/2022 01:09:47 AM TRAINING: Best xgb classifier trained
INFO 06/04/2022 01:09:47 AM TRAINING: Best xgb classifier trained
INFO 06/04/2022 01:09:47 AM TRAINING: Search for best xgb classifier model ended
INFO 06/04/2022 01:09:47 AM TRAINING: Search for best xgb classifier model ended
INFO 06/04/2022 01:09:47 AM TRAINING: Search for best xgb classifier model ended
INFO 06/04/2022 01:09:47 AM TRAINING: The best model is svc
INFO 06/04/2022 01:09:47 AM TRAINING: The best model is svc
INFO 06/04/2022 01:09:47 AM TRAINING: The best model is svc
INFO 06/04/2022 09:28:00 AM TRAINING: Search for best model started
INFO 06/04/2022 09:28:00 AM TRAINING: Search for best logistic regressor model started
INFO 06/04/2022 09:28:00 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/04/2022 09:28:36 AM TRAINING: Search for best model started
INFO 06/04/2022 09:28:36 AM TRAINING: Search for best logistic regressor model started
INFO 06/04/2022 09:28:36 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/04/2022 09:28:46 AM TRAINING: The optimum parameters of Logistic Regressor are C=0.01, penalty=l1,solver=saga  with the f1 score of 0.0
INFO 06/04/2022 09:28:46 AM TRAINING: Best Logistic  Regressor trained
INFO 06/04/2022 09:28:46 AM TRAINING: Search for best ridge model ended
INFO 06/04/2022 09:28:46 AM TRAINING: Search for best svc model started
INFO 06/04/2022 09:28:46 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/04/2022 09:35:52 AM TRAINING: The optimum parameters of SVC are kernel=sigmoid, gamma=scale, C=10, degree =2 with the f1 score of 0.19133165353884438
INFO 06/04/2022 09:35:54 AM TRAINING: Best SVC trained
INFO 06/04/2022 09:35:54 AM TRAINING: Search for best svc model ended
INFO 06/04/2022 09:35:54 AM TRAINING: Search for best random forest classifier model started
INFO 06/04/2022 09:35:54 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/04/2022 10:17:01 AM TRAINING: The optimum parameters of random forrest classifier are n_estimators=100, criterion=gini, min_samples_split=3, max_features =sqrt, ccp_alpha=0.0 with the adjusted R2 score of 0.14188532625799244
INFO 06/04/2022 10:17:02 AM TRAINING: Best random forest classifier trained
INFO 06/04/2022 10:17:02 AM TRAINING: Search for best random forest classifier model ended
INFO 06/04/2022 10:17:02 AM TRAINING: Search for best xgb classifier model started
INFO 06/04/2022 10:17:02 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/04/2022 11:24:58 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=1, max_depth=3, colsample_bytree=0.1, n_estimators =3000 with the adjusted R2 score of 0.20064220015758777
INFO 06/04/2022 11:25:11 AM TRAINING: Best xgb classifier trained
INFO 06/04/2022 11:25:11 AM TRAINING: Search for best xgb classifier model ended
INFO 06/04/2022 11:25:11 AM TRAINING: The best model is random forest classifier
INFO 06/04/2022 11:25:11 AM TRAINING: Search for best model started
INFO 06/04/2022 11:25:11 AM TRAINING: Search for best model started
INFO 06/04/2022 11:25:11 AM TRAINING: Search for best logistic regressor model started
INFO 06/04/2022 11:25:11 AM TRAINING: Search for best logistic regressor model started
INFO 06/04/2022 11:25:11 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/04/2022 11:25:11 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/04/2022 11:25:16 AM TRAINING: The optimum parameters of Logistic Regressor are C=3, penalty=l1,solver=saga  with the f1 score of 0.5176515101064981
INFO 06/04/2022 11:25:16 AM TRAINING: The optimum parameters of Logistic Regressor are C=3, penalty=l1,solver=saga  with the f1 score of 0.5176515101064981
INFO 06/04/2022 11:25:16 AM TRAINING: Best Logistic  Regressor trained
INFO 06/04/2022 11:25:16 AM TRAINING: Best Logistic  Regressor trained
INFO 06/04/2022 11:25:16 AM TRAINING: Search for best ridge model ended
INFO 06/04/2022 11:25:16 AM TRAINING: Search for best ridge model ended
INFO 06/04/2022 11:25:16 AM TRAINING: Search for best svc model started
INFO 06/04/2022 11:25:16 AM TRAINING: Search for best svc model started
INFO 06/04/2022 11:25:16 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/04/2022 11:25:16 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/04/2022 07:57:18 PM TRAINING: Search for best model started
INFO 06/04/2022 07:57:18 PM TRAINING: Search for best logistic regressor model started
INFO 06/04/2022 07:57:18 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/04/2022 07:57:30 PM TRAINING: The optimum parameters of Logistic Regressor are C=10, penalty=l1,solver=saga  with the f1 score of 0.483192859007832
INFO 06/04/2022 07:57:30 PM TRAINING: Best Logistic  Regressor trained
INFO 06/04/2022 07:57:30 PM TRAINING: Search for best ridge model ended
INFO 06/04/2022 07:57:30 PM TRAINING: Search for best svc model started
INFO 06/04/2022 07:57:30 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/04/2022 07:58:29 PM TRAINING: Search for best model started
INFO 06/04/2022 07:58:29 PM TRAINING: Search for best logistic regressor model started
INFO 06/04/2022 07:58:29 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/04/2022 07:58:40 PM TRAINING: The optimum parameters of Logistic Regressor are C=3, penalty=l2,solver=newton-cg  with the f1 score of 0.5034434529482585
INFO 06/04/2022 08:00:00 PM TRAINING: Search for best model started
INFO 06/04/2022 08:00:00 PM TRAINING: Search for best logistic regressor model started
INFO 06/04/2022 08:00:00 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/04/2022 08:00:11 PM TRAINING: The optimum parameters of Logistic Regressor are C=10, penalty=l2,solver=newton-cg  with the f1 score of 0.49290174246422663
INFO 06/04/2022 08:00:11 PM TRAINING: Best Logistic  Regressor trained
INFO 06/04/2022 08:00:11 PM TRAINING: Search for best ridge model ended
INFO 06/04/2022 08:00:11 PM TRAINING: Search for best svc model started
INFO 06/04/2022 08:00:11 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/04/2022 08:18:28 PM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=auto, C=10, degree =2 with the f1 score of 0.5729256910392224
INFO 06/04/2022 08:18:32 PM TRAINING: Best SVC trained
INFO 06/04/2022 08:18:34 PM TRAINING: Search for best svc model ended
INFO 06/04/2022 08:18:34 PM TRAINING: Search for best random forest classifier model started
INFO 06/04/2022 08:18:34 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/04/2022 08:49:50 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=130, criterion=entropy, min_samples_split=3, max_features =auto, ccp_alpha=0.0 with the adjusted R2 score of 0.6143846334073064
INFO 06/04/2022 08:49:52 PM TRAINING: Best random forest classifier trained
INFO 06/04/2022 08:49:52 PM TRAINING: Search for best random forest classifier model ended
INFO 06/04/2022 08:49:52 PM TRAINING: Search for best xgb classifier model started
INFO 06/04/2022 08:49:52 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/04/2022 09:59:01 PM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.1, max_depth=10, colsample_bytree=0.7, n_estimators =300 with the adjusted R2 score of 0.631546513299255
INFO 06/04/2022 09:59:04 PM TRAINING: Best xgb classifier trained
INFO 06/04/2022 09:59:04 PM TRAINING: Search for best xgb classifier model ended
INFO 06/04/2022 09:59:04 PM TRAINING: The best model is xgb classifier
INFO 06/04/2022 09:59:04 PM TRAINING: Search for best model started
INFO 06/04/2022 09:59:04 PM TRAINING: Search for best model started
INFO 06/04/2022 09:59:04 PM TRAINING: Search for best logistic regressor model started
INFO 06/04/2022 09:59:04 PM TRAINING: Search for best logistic regressor model started
INFO 06/04/2022 09:59:04 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/04/2022 09:59:04 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/04/2022 09:59:10 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.01, penalty=l2,solver=newton-cg  with the f1 score of 0.7403955909096952
INFO 06/04/2022 09:59:10 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.01, penalty=l2,solver=newton-cg  with the f1 score of 0.7403955909096952
INFO 06/04/2022 09:59:10 PM TRAINING: Best Logistic  Regressor trained
INFO 06/04/2022 09:59:10 PM TRAINING: Best Logistic  Regressor trained
INFO 06/04/2022 09:59:10 PM TRAINING: Search for best ridge model ended
INFO 06/04/2022 09:59:10 PM TRAINING: Search for best ridge model ended
INFO 06/04/2022 09:59:10 PM TRAINING: Search for best svc model started
INFO 06/04/2022 09:59:10 PM TRAINING: Search for best svc model started
INFO 06/04/2022 09:59:10 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/04/2022 09:59:10 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/04/2022 10:55:50 PM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=scale, C=3, degree =2 with the f1 score of 0.7610010079018672
INFO 06/04/2022 10:55:50 PM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=scale, C=3, degree =2 with the f1 score of 0.7610010079018672
INFO 06/04/2022 10:56:00 PM TRAINING: Best SVC trained
INFO 06/04/2022 10:56:00 PM TRAINING: Best SVC trained
INFO 06/04/2022 10:56:06 PM TRAINING: Search for best svc model ended
INFO 06/04/2022 10:56:06 PM TRAINING: Search for best svc model ended
INFO 06/04/2022 10:56:06 PM TRAINING: Search for best random forest classifier model started
INFO 06/04/2022 10:56:06 PM TRAINING: Search for best random forest classifier model started
INFO 06/04/2022 10:56:06 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/04/2022 10:56:06 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/04/2022 11:52:13 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=300, criterion=entropy, min_samples_split=2, max_features =auto, ccp_alpha=0.0 with the adjusted R2 score of 0.7821037916721254
INFO 06/04/2022 11:52:13 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=300, criterion=entropy, min_samples_split=2, max_features =auto, ccp_alpha=0.0 with the adjusted R2 score of 0.7821037916721254
INFO 06/04/2022 11:52:18 PM TRAINING: Best random forest classifier trained
INFO 06/04/2022 11:52:18 PM TRAINING: Best random forest classifier trained
INFO 06/04/2022 11:52:18 PM TRAINING: Search for best random forest classifier model ended
INFO 06/04/2022 11:52:18 PM TRAINING: Search for best random forest classifier model ended
INFO 06/04/2022 11:52:18 PM TRAINING: Search for best xgb classifier model started
INFO 06/04/2022 11:52:18 PM TRAINING: Search for best xgb classifier model started
INFO 06/04/2022 11:52:18 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/04/2022 11:52:18 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/05/2022 01:42:59 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.01, max_depth=20, colsample_bytree=0.5, n_estimators =1000 with the adjusted R2 score of 0.7867380237824123
INFO 06/05/2022 01:42:59 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.01, max_depth=20, colsample_bytree=0.5, n_estimators =1000 with the adjusted R2 score of 0.7867380237824123
INFO 06/05/2022 01:43:25 AM TRAINING: Best xgb classifier trained
INFO 06/05/2022 01:43:25 AM TRAINING: Best xgb classifier trained
INFO 06/05/2022 01:43:26 AM TRAINING: Search for best xgb classifier model ended
INFO 06/05/2022 01:43:26 AM TRAINING: Search for best xgb classifier model ended
INFO 06/05/2022 01:43:26 AM TRAINING: The best model is xgb classifier
INFO 06/05/2022 01:43:26 AM TRAINING: The best model is xgb classifier
INFO 06/05/2022 01:43:26 AM TRAINING: Search for best model started
INFO 06/05/2022 01:43:26 AM TRAINING: Search for best model started
INFO 06/05/2022 01:43:26 AM TRAINING: Search for best model started
INFO 06/05/2022 01:43:26 AM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 01:43:26 AM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 01:43:26 AM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 01:43:26 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 01:43:26 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 01:43:26 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 01:43:35 AM TRAINING: The optimum parameters of Logistic Regressor are C=0.1, penalty=l2,solver=newton-cg  with the f1 score of 0.6621712561872698
INFO 06/05/2022 01:43:35 AM TRAINING: The optimum parameters of Logistic Regressor are C=0.1, penalty=l2,solver=newton-cg  with the f1 score of 0.6621712561872698
INFO 06/05/2022 01:43:35 AM TRAINING: The optimum parameters of Logistic Regressor are C=0.1, penalty=l2,solver=newton-cg  with the f1 score of 0.6621712561872698
INFO 06/05/2022 01:43:35 AM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 01:43:35 AM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 01:43:35 AM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 01:43:35 AM TRAINING: Search for best ridge model ended
INFO 06/05/2022 01:43:35 AM TRAINING: Search for best ridge model ended
INFO 06/05/2022 01:43:35 AM TRAINING: Search for best ridge model ended
INFO 06/05/2022 01:43:35 AM TRAINING: Search for best svc model started
INFO 06/05/2022 01:43:35 AM TRAINING: Search for best svc model started
INFO 06/05/2022 01:43:35 AM TRAINING: Search for best svc model started
INFO 06/05/2022 01:43:35 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 01:43:35 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 01:43:35 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 01:56:18 AM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=auto, C=10, degree =2 with the f1 score of 0.7304862674449875
INFO 06/05/2022 01:56:18 AM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=auto, C=10, degree =2 with the f1 score of 0.7304862674449875
INFO 06/05/2022 01:56:18 AM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=auto, C=10, degree =2 with the f1 score of 0.7304862674449875
INFO 06/05/2022 01:56:19 AM TRAINING: Best SVC trained
INFO 06/05/2022 01:56:19 AM TRAINING: Best SVC trained
INFO 06/05/2022 01:56:19 AM TRAINING: Best SVC trained
INFO 06/05/2022 01:56:19 AM TRAINING: Search for best svc model ended
INFO 06/05/2022 01:56:19 AM TRAINING: Search for best svc model ended
INFO 06/05/2022 01:56:19 AM TRAINING: Search for best svc model ended
INFO 06/05/2022 01:56:19 AM TRAINING: Search for best random forest classifier model started
INFO 06/05/2022 01:56:19 AM TRAINING: Search for best random forest classifier model started
INFO 06/05/2022 01:56:19 AM TRAINING: Search for best random forest classifier model started
INFO 06/05/2022 01:56:19 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/05/2022 01:56:19 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/05/2022 01:56:19 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/05/2022 02:08:26 AM TRAINING: The optimum parameters of random forrest classifier are n_estimators=150, criterion=gini, min_samples_split=3, max_features =log2, ccp_alpha=0.0 with the adjusted R2 score of 0.7517421639585636
INFO 06/05/2022 02:08:26 AM TRAINING: The optimum parameters of random forrest classifier are n_estimators=150, criterion=gini, min_samples_split=3, max_features =log2, ccp_alpha=0.0 with the adjusted R2 score of 0.7517421639585636
INFO 06/05/2022 02:08:26 AM TRAINING: The optimum parameters of random forrest classifier are n_estimators=150, criterion=gini, min_samples_split=3, max_features =log2, ccp_alpha=0.0 with the adjusted R2 score of 0.7517421639585636
INFO 06/05/2022 02:08:26 AM TRAINING: Best random forest classifier trained
INFO 06/05/2022 02:08:26 AM TRAINING: Best random forest classifier trained
INFO 06/05/2022 02:08:26 AM TRAINING: Best random forest classifier trained
INFO 06/05/2022 02:08:26 AM TRAINING: Search for best random forest classifier model ended
INFO 06/05/2022 02:08:26 AM TRAINING: Search for best random forest classifier model ended
INFO 06/05/2022 02:08:26 AM TRAINING: Search for best random forest classifier model ended
INFO 06/05/2022 02:08:26 AM TRAINING: Search for best xgb classifier model started
INFO 06/05/2022 02:08:26 AM TRAINING: Search for best xgb classifier model started
INFO 06/05/2022 02:08:26 AM TRAINING: Search for best xgb classifier model started
INFO 06/05/2022 02:08:26 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/05/2022 02:08:26 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/05/2022 02:08:26 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/05/2022 02:31:11 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.01, max_depth=15, colsample_bytree=0.9, n_estimators =3000 with the adjusted R2 score of 0.7658924852808399
INFO 06/05/2022 02:31:11 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.01, max_depth=15, colsample_bytree=0.9, n_estimators =3000 with the adjusted R2 score of 0.7658924852808399
INFO 06/05/2022 02:31:11 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.01, max_depth=15, colsample_bytree=0.9, n_estimators =3000 with the adjusted R2 score of 0.7658924852808399
INFO 06/05/2022 02:31:30 AM TRAINING: Best xgb classifier trained
INFO 06/05/2022 02:31:30 AM TRAINING: Best xgb classifier trained
INFO 06/05/2022 02:31:30 AM TRAINING: Best xgb classifier trained
INFO 06/05/2022 02:31:30 AM TRAINING: Search for best xgb classifier model ended
INFO 06/05/2022 02:31:30 AM TRAINING: Search for best xgb classifier model ended
INFO 06/05/2022 02:31:30 AM TRAINING: Search for best xgb classifier model ended
INFO 06/05/2022 02:31:30 AM TRAINING: The best model is xgb classifier
INFO 06/05/2022 02:31:30 AM TRAINING: The best model is xgb classifier
INFO 06/05/2022 02:31:30 AM TRAINING: The best model is xgb classifier
INFO 06/05/2022 05:52:09 AM TRAINING: Search for best model started
INFO 06/05/2022 05:52:09 AM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 05:52:09 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 05:52:19 AM TRAINING: The optimum parameters of Logistic Regressor are C=1, penalty=l1,solver=saga  with the f1 score of 0.5595588439956183
INFO 06/05/2022 05:52:20 AM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 05:52:20 AM TRAINING: Search for best ridge model ended
INFO 06/05/2022 05:52:20 AM TRAINING: Search for best svc model started
INFO 06/05/2022 05:52:20 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 05:53:25 AM TRAINING: Search for best model started
INFO 06/05/2022 05:53:25 AM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 05:53:25 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 05:53:32 AM TRAINING: The optimum parameters of Logistic Regressor are C=1, penalty=l2,solver=newton-cg  with the f1 score of 0.5525224398498478
INFO 06/05/2022 05:53:32 AM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 05:53:32 AM TRAINING: Search for best ridge model ended
INFO 06/05/2022 05:53:32 AM TRAINING: Search for best svc model started
INFO 06/05/2022 05:53:32 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 06:12:22 AM TRAINING: The optimum parameters of SVC are kernel=poly, gamma=auto, C=10, degree =4 with the f1 score of 0.6447188228833222
INFO 06/05/2022 06:12:32 AM TRAINING: Best SVC trained
INFO 06/05/2022 06:12:32 AM TRAINING: Search for best svc model ended
INFO 06/05/2022 06:12:32 AM TRAINING: Search for best random forest classifier model started
INFO 06/05/2022 06:12:32 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/05/2022 06:44:26 AM TRAINING: The optimum parameters of random forrest classifier are n_estimators=150, criterion=entropy, min_samples_split=3, max_features =auto, ccp_alpha=0.0 with the adjusted R2 score of 0.6475769808524465
INFO 06/05/2022 06:44:27 AM TRAINING: Best random forest classifier trained
INFO 06/05/2022 06:44:28 AM TRAINING: Search for best random forest classifier model ended
INFO 06/05/2022 06:44:28 AM TRAINING: Search for best xgb classifier model started
INFO 06/05/2022 06:44:28 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/05/2022 07:58:28 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=20, colsample_bytree=0.8, n_estimators =3000 with the adjusted R2 score of 0.6767688850801277
INFO 06/05/2022 07:59:14 AM TRAINING: Best xgb classifier trained
INFO 06/05/2022 07:59:14 AM TRAINING: Search for best xgb classifier model ended
INFO 06/05/2022 07:59:14 AM TRAINING: The best model is xgb classifier
INFO 06/05/2022 07:59:14 AM TRAINING: Search for best model started
INFO 06/05/2022 07:59:14 AM TRAINING: Search for best model started
INFO 06/05/2022 07:59:14 AM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 07:59:14 AM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 07:59:14 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 07:59:14 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 07:59:19 AM TRAINING: The optimum parameters of Logistic Regressor are C=0.1, penalty=l1,solver=saga  with the f1 score of 0.7564434149995878
INFO 06/05/2022 07:59:19 AM TRAINING: The optimum parameters of Logistic Regressor are C=0.1, penalty=l1,solver=saga  with the f1 score of 0.7564434149995878
INFO 06/05/2022 07:59:20 AM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 07:59:20 AM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 07:59:20 AM TRAINING: Search for best ridge model ended
INFO 06/05/2022 07:59:20 AM TRAINING: Search for best ridge model ended
INFO 06/05/2022 07:59:20 AM TRAINING: Search for best svc model started
INFO 06/05/2022 07:59:20 AM TRAINING: Search for best svc model started
INFO 06/05/2022 07:59:20 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 07:59:20 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 01:20:44 PM TRAINING: Search for best model started
INFO 06/05/2022 01:20:44 PM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 01:20:44 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 01:20:54 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.03, penalty=l2,solver=newton-cg  with the f1 score of 0.6030255931271721
INFO 06/05/2022 01:20:54 PM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 01:20:54 PM TRAINING: Search for best ridge model ended
INFO 06/05/2022 01:20:54 PM TRAINING: Search for best svc model started
INFO 06/05/2022 01:20:54 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 02:01:51 PM TRAINING: The optimum parameters of SVC are kernel=poly, gamma=auto, C=10, degree =4 with the f1 score of 0.6697843424618087
INFO 06/05/2022 02:01:59 PM TRAINING: Best SVC trained
INFO 06/05/2022 02:01:59 PM TRAINING: Search for best svc model ended
INFO 06/05/2022 02:01:59 PM TRAINING: Search for best random forest classifier model started
INFO 06/05/2022 02:01:59 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/05/2022 02:10:23 PM TRAINING: Search for best model started
INFO 06/05/2022 02:10:23 PM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 02:10:23 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 02:10:32 PM TRAINING: The optimum parameters of Logistic Regressor are C=3, penalty=l1,solver=saga  with the f1 score of 0.6083039085949402
INFO 06/05/2022 02:10:32 PM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 02:10:32 PM TRAINING: Search for best ridge model ended
INFO 06/05/2022 02:10:32 PM TRAINING: Search for best svc model started
INFO 06/05/2022 02:10:32 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 03:07:50 PM TRAINING: The optimum parameters of SVC are kernel=poly, gamma=auto, C=10, degree =5 with the f1 score of 0.672582069948916
INFO 06/05/2022 03:07:59 PM TRAINING: Best SVC trained
INFO 06/05/2022 03:07:59 PM TRAINING: Search for best svc model ended
INFO 06/05/2022 03:07:59 PM TRAINING: Search for best random forest classifier model started
INFO 06/05/2022 03:07:59 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/05/2022 03:09:47 PM TRAINING: Search for best model started
INFO 06/05/2022 03:09:47 PM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 03:09:47 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 03:09:57 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.03, penalty=l2,solver=newton-cg  with the f1 score of 0.5902738685038279
INFO 06/05/2022 03:09:57 PM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 03:09:57 PM TRAINING: Search for best ridge model ended
INFO 06/05/2022 03:09:57 PM TRAINING: Search for best svc model started
INFO 06/05/2022 03:09:57 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 03:12:48 PM TRAINING: Search for best model started
INFO 06/05/2022 03:12:48 PM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 03:12:48 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 03:12:59 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.01, penalty=l2,solver=newton-cg  with the f1 score of 0.5991609519821244
INFO 06/05/2022 03:12:59 PM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 03:12:59 PM TRAINING: Search for best ridge model ended
INFO 06/05/2022 03:12:59 PM TRAINING: Search for best svc model started
INFO 06/05/2022 03:12:59 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 04:02:03 PM TRAINING: Search for best model started
INFO 06/05/2022 04:02:03 PM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 04:02:03 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 04:02:14 PM TRAINING: The optimum parameters of Logistic Regressor are C=3, penalty=l2,solver=saga  with the f1 score of 0.41782262698101047
INFO 06/05/2022 04:02:14 PM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 04:02:14 PM TRAINING: Search for best ridge model ended
INFO 06/05/2022 04:02:14 PM TRAINING: Search for best svc model started
INFO 06/05/2022 04:02:14 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 04:03:42 PM TRAINING: Search for best model started
INFO 06/05/2022 04:03:42 PM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 04:03:42 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 04:03:53 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.03, penalty=l2,solver=newton-cg  with the f1 score of 0.6129291876719798
INFO 06/05/2022 04:03:53 PM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 04:03:53 PM TRAINING: Search for best ridge model ended
INFO 06/05/2022 04:03:53 PM TRAINING: Search for best svc model started
INFO 06/05/2022 04:03:53 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 04:05:11 PM TRAINING: Search for best model started
INFO 06/05/2022 04:05:11 PM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 04:05:11 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 04:05:22 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.1, penalty=l2,solver=newton-cg  with the f1 score of 0.614705821365366
INFO 06/05/2022 04:05:23 PM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 04:05:23 PM TRAINING: Search for best ridge model ended
INFO 06/05/2022 04:05:23 PM TRAINING: Search for best svc model started
INFO 06/05/2022 04:05:23 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 05:02:51 PM TRAINING: The optimum parameters of SVC are kernel=poly, gamma=auto, C=10, degree =5 with the f1 score of 0.6705219644784923
INFO 06/05/2022 05:03:01 PM TRAINING: Best SVC trained
INFO 06/05/2022 05:03:01 PM TRAINING: Search for best svc model ended
INFO 06/05/2022 05:03:01 PM TRAINING: Search for best random forest classifier model started
INFO 06/05/2022 05:03:01 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/05/2022 05:49:44 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=300, criterion=entropy, min_samples_split=4, max_features =log2, ccp_alpha=0.0 with the adjusted R2 score of 0.6883462885681831
INFO 06/05/2022 05:49:48 PM TRAINING: Best random forest classifier trained
INFO 06/05/2022 05:49:48 PM TRAINING: Search for best random forest classifier model ended
INFO 06/05/2022 05:49:48 PM TRAINING: Search for best xgb classifier model started
INFO 06/05/2022 05:49:48 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/05/2022 08:04:27 PM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.1, max_depth=15, colsample_bytree=0.8, n_estimators =1000 with the adjusted R2 score of 0.7088759045864146
INFO 06/05/2022 08:04:53 PM TRAINING: Best xgb classifier trained
INFO 06/05/2022 08:04:53 PM TRAINING: Search for best xgb classifier model ended
INFO 06/05/2022 08:04:53 PM TRAINING: The best model is xgb classifier
INFO 06/05/2022 08:04:53 PM TRAINING: Search for best model started
INFO 06/05/2022 08:04:53 PM TRAINING: Search for best model started
INFO 06/05/2022 08:04:53 PM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 08:04:53 PM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 08:04:53 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 08:04:53 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 08:05:02 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.01, penalty=l2,solver=saga  with the f1 score of 0.7879497838526308
INFO 06/05/2022 08:05:02 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.01, penalty=l2,solver=saga  with the f1 score of 0.7879497838526308
INFO 06/05/2022 08:05:02 PM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 08:05:02 PM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 08:05:02 PM TRAINING: Search for best ridge model ended
INFO 06/05/2022 08:05:02 PM TRAINING: Search for best ridge model ended
INFO 06/05/2022 08:05:02 PM TRAINING: Search for best svc model started
INFO 06/05/2022 08:05:02 PM TRAINING: Search for best svc model started
INFO 06/05/2022 08:05:02 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 08:05:02 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 08:39:31 PM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=scale, C=3, degree =2 with the f1 score of 0.8066204407023021
INFO 06/05/2022 08:39:31 PM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=scale, C=3, degree =2 with the f1 score of 0.8066204407023021
INFO 06/05/2022 08:39:37 PM TRAINING: Best SVC trained
INFO 06/05/2022 08:39:37 PM TRAINING: Best SVC trained
INFO 06/05/2022 08:39:40 PM TRAINING: Search for best svc model ended
INFO 06/05/2022 08:39:40 PM TRAINING: Search for best svc model ended
INFO 06/05/2022 08:39:40 PM TRAINING: Search for best random forest classifier model started
INFO 06/05/2022 08:39:40 PM TRAINING: Search for best random forest classifier model started
INFO 06/05/2022 08:39:40 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/05/2022 08:39:40 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/05/2022 09:35:52 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=130, criterion=gini, min_samples_split=2, max_features =sqrt, ccp_alpha=0.0 with the adjusted R2 score of 0.8186203975414065
INFO 06/05/2022 09:35:52 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=130, criterion=gini, min_samples_split=2, max_features =sqrt, ccp_alpha=0.0 with the adjusted R2 score of 0.8186203975414065
INFO 06/05/2022 09:35:54 PM TRAINING: Best random forest classifier trained
INFO 06/05/2022 09:35:54 PM TRAINING: Best random forest classifier trained
INFO 06/05/2022 09:35:54 PM TRAINING: Search for best random forest classifier model ended
INFO 06/05/2022 09:35:54 PM TRAINING: Search for best random forest classifier model ended
INFO 06/05/2022 09:35:54 PM TRAINING: Search for best xgb classifier model started
INFO 06/05/2022 09:35:54 PM TRAINING: Search for best xgb classifier model started
INFO 06/05/2022 09:35:54 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/05/2022 09:35:54 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/05/2022 11:51:22 PM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=20, colsample_bytree=0.5, n_estimators =300 with the adjusted R2 score of 0.8203698064920395
INFO 06/05/2022 11:51:22 PM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=20, colsample_bytree=0.5, n_estimators =300 with the adjusted R2 score of 0.8203698064920395
INFO 06/05/2022 11:51:32 PM TRAINING: Best xgb classifier trained
INFO 06/05/2022 11:51:32 PM TRAINING: Best xgb classifier trained
INFO 06/05/2022 11:51:32 PM TRAINING: Search for best xgb classifier model ended
INFO 06/05/2022 11:51:32 PM TRAINING: Search for best xgb classifier model ended
INFO 06/05/2022 11:51:32 PM TRAINING: The best model is xgb classifier
INFO 06/05/2022 11:51:32 PM TRAINING: The best model is xgb classifier
INFO 06/05/2022 11:51:32 PM TRAINING: Search for best model started
INFO 06/05/2022 11:51:32 PM TRAINING: Search for best model started
INFO 06/05/2022 11:51:32 PM TRAINING: Search for best model started
INFO 06/05/2022 11:51:32 PM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 11:51:32 PM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 11:51:32 PM TRAINING: Search for best logistic regressor model started
INFO 06/05/2022 11:51:32 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 11:51:32 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 11:51:32 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/05/2022 11:51:39 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.3, penalty=l2,solver=sag  with the f1 score of 0.7316187999026674
INFO 06/05/2022 11:51:39 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.3, penalty=l2,solver=sag  with the f1 score of 0.7316187999026674
INFO 06/05/2022 11:51:39 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.3, penalty=l2,solver=sag  with the f1 score of 0.7316187999026674
INFO 06/05/2022 11:51:39 PM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 11:51:39 PM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 11:51:39 PM TRAINING: Best Logistic  Regressor trained
INFO 06/05/2022 11:51:39 PM TRAINING: Search for best ridge model ended
INFO 06/05/2022 11:51:39 PM TRAINING: Search for best ridge model ended
INFO 06/05/2022 11:51:39 PM TRAINING: Search for best ridge model ended
INFO 06/05/2022 11:51:39 PM TRAINING: Search for best svc model started
INFO 06/05/2022 11:51:39 PM TRAINING: Search for best svc model started
INFO 06/05/2022 11:51:39 PM TRAINING: Search for best svc model started
INFO 06/05/2022 11:51:39 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 11:51:39 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/05/2022 11:51:39 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/06/2022 12:13:45 AM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=auto, C=10, degree =2 with the f1 score of 0.7519787823937485
INFO 06/06/2022 12:13:45 AM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=auto, C=10, degree =2 with the f1 score of 0.7519787823937485
INFO 06/06/2022 12:13:45 AM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=auto, C=10, degree =2 with the f1 score of 0.7519787823937485
INFO 06/06/2022 12:13:48 AM TRAINING: Best SVC trained
INFO 06/06/2022 12:13:48 AM TRAINING: Best SVC trained
INFO 06/06/2022 12:13:48 AM TRAINING: Best SVC trained
INFO 06/06/2022 12:13:49 AM TRAINING: Search for best svc model ended
INFO 06/06/2022 12:13:49 AM TRAINING: Search for best svc model ended
INFO 06/06/2022 12:13:49 AM TRAINING: Search for best svc model ended
INFO 06/06/2022 12:13:49 AM TRAINING: Search for best random forest classifier model started
INFO 06/06/2022 12:13:49 AM TRAINING: Search for best random forest classifier model started
INFO 06/06/2022 12:13:49 AM TRAINING: Search for best random forest classifier model started
INFO 06/06/2022 12:13:49 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/06/2022 12:13:49 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/06/2022 12:13:49 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/06/2022 12:46:35 AM TRAINING: The optimum parameters of random forrest classifier are n_estimators=100, criterion=entropy, min_samples_split=3, max_features =auto, ccp_alpha=0.0 with the adjusted R2 score of 0.7666390080923426
INFO 06/06/2022 12:46:35 AM TRAINING: The optimum parameters of random forrest classifier are n_estimators=100, criterion=entropy, min_samples_split=3, max_features =auto, ccp_alpha=0.0 with the adjusted R2 score of 0.7666390080923426
INFO 06/06/2022 12:46:35 AM TRAINING: The optimum parameters of random forrest classifier are n_estimators=100, criterion=entropy, min_samples_split=3, max_features =auto, ccp_alpha=0.0 with the adjusted R2 score of 0.7666390080923426
INFO 06/06/2022 12:46:36 AM TRAINING: Best random forest classifier trained
INFO 06/06/2022 12:46:36 AM TRAINING: Best random forest classifier trained
INFO 06/06/2022 12:46:36 AM TRAINING: Best random forest classifier trained
INFO 06/06/2022 12:46:36 AM TRAINING: Search for best random forest classifier model ended
INFO 06/06/2022 12:46:36 AM TRAINING: Search for best random forest classifier model ended
INFO 06/06/2022 12:46:36 AM TRAINING: Search for best random forest classifier model ended
INFO 06/06/2022 12:46:36 AM TRAINING: Search for best xgb classifier model started
INFO 06/06/2022 12:46:36 AM TRAINING: Search for best xgb classifier model started
INFO 06/06/2022 12:46:36 AM TRAINING: Search for best xgb classifier model started
INFO 06/06/2022 12:46:36 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/06/2022 12:46:36 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/06/2022 12:46:36 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/06/2022 01:57:18 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=15, colsample_bytree=0.7, n_estimators =3000 with the adjusted R2 score of 0.7824472873774599
INFO 06/06/2022 01:57:18 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=15, colsample_bytree=0.7, n_estimators =3000 with the adjusted R2 score of 0.7824472873774599
INFO 06/06/2022 01:57:18 AM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=15, colsample_bytree=0.7, n_estimators =3000 with the adjusted R2 score of 0.7824472873774599
INFO 06/06/2022 01:57:53 AM TRAINING: Best xgb classifier trained
INFO 06/06/2022 01:57:53 AM TRAINING: Best xgb classifier trained
INFO 06/06/2022 01:57:53 AM TRAINING: Best xgb classifier trained
INFO 06/06/2022 01:57:53 AM TRAINING: Search for best xgb classifier model ended
INFO 06/06/2022 01:57:53 AM TRAINING: Search for best xgb classifier model ended
INFO 06/06/2022 01:57:53 AM TRAINING: Search for best xgb classifier model ended
INFO 06/06/2022 01:57:53 AM TRAINING: The best model is xgb classifier
INFO 06/06/2022 01:57:53 AM TRAINING: The best model is xgb classifier
INFO 06/06/2022 01:57:53 AM TRAINING: The best model is xgb classifier
