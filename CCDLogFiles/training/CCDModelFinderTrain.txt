INFO 06/06/2022 07:39:40 AM TRAINING: Search for best model started
INFO 06/06/2022 07:39:40 AM TRAINING: Search for best logistic regressor model started
INFO 06/06/2022 07:39:40 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/06/2022 07:39:52 AM TRAINING: The optimum parameters of Logistic Regressor are C=0.3, penalty=l2,solver=newton-cg  with the f1 score of 0.6008026663325092
INFO 06/06/2022 07:39:52 AM TRAINING: Best Logistic  Regressor trained
INFO 06/06/2022 07:39:52 AM TRAINING: Search for best ridge model ended
INFO 06/06/2022 07:39:52 AM TRAINING: Search for best svc model started
INFO 06/06/2022 07:39:52 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/06/2022 07:47:09 AM TRAINING: Search for best model started
INFO 06/06/2022 07:47:09 AM TRAINING: Search for best logistic regressor model started
INFO 06/06/2022 07:47:09 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/06/2022 07:47:22 AM TRAINING: The optimum parameters of Logistic Regressor are C=3, penalty=l2,solver=newton-cg  with the f1 score of 0.5907610192501308
INFO 06/06/2022 07:47:22 AM TRAINING: Best Logistic  Regressor trained
INFO 06/06/2022 07:47:22 AM TRAINING: Search for best ridge model ended
INFO 06/06/2022 07:47:22 AM TRAINING: Search for best svc model started
INFO 06/06/2022 07:47:22 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/06/2022 08:34:00 AM TRAINING: The optimum parameters of SVC are kernel=poly, gamma=auto, C=10, degree =5 with the f1 score of 0.6667670115865609
INFO 06/06/2022 08:34:10 AM TRAINING: Best SVC trained
INFO 06/06/2022 08:34:11 AM TRAINING: Search for best svc model ended
INFO 06/06/2022 08:34:11 AM TRAINING: Search for best random forest classifier model started
INFO 06/06/2022 08:34:11 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/06/2022 09:17:19 AM TRAINING: The optimum parameters of random forrest classifier are n_estimators=150, criterion=entropy, min_samples_split=3, max_features =sqrt, ccp_alpha=0.0 with the adjusted R2 score of 0.6840102693758572
INFO 06/06/2022 09:17:21 AM TRAINING: Best random forest classifier trained
INFO 06/06/2022 09:17:21 AM TRAINING: Search for best random forest classifier model ended
INFO 06/06/2022 09:17:21 AM TRAINING: Search for best xgb classifier model started
INFO 06/06/2022 09:17:21 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/06/2022 12:40:21 PM TRAINING: Search for best model started
INFO 06/06/2022 12:40:21 PM TRAINING: Search for best logistic regressor model started
INFO 06/06/2022 12:40:21 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/06/2022 12:40:31 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.03, penalty=l2,solver=newton-cg  with the f1 score of 0.5983392668745953
INFO 06/06/2022 12:40:31 PM TRAINING: Best Logistic  Regressor trained
INFO 06/06/2022 12:40:31 PM TRAINING: Search for best ridge model ended
INFO 06/06/2022 12:40:31 PM TRAINING: Search for best svc model started
INFO 06/06/2022 12:40:31 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/06/2022 01:32:08 PM TRAINING: The optimum parameters of SVC are kernel=poly, gamma=auto, C=10, degree =4 with the f1 score of 0.6602348986939857
INFO 06/06/2022 01:32:15 PM TRAINING: Best SVC trained
INFO 06/06/2022 01:32:16 PM TRAINING: Search for best svc model ended
INFO 06/06/2022 01:32:16 PM TRAINING: Search for best random forest classifier model started
INFO 06/06/2022 01:32:16 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/06/2022 02:19:07 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=150, criterion=gini, min_samples_split=3, max_features =sqrt, ccp_alpha=0.0 with the adjusted R2 score of 0.6719175535233283
INFO 06/06/2022 02:19:08 PM TRAINING: Best random forest classifier trained
INFO 06/06/2022 02:19:08 PM TRAINING: Search for best random forest classifier model ended
INFO 06/06/2022 02:19:08 PM TRAINING: Search for best xgb classifier model started
INFO 06/06/2022 02:19:08 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/06/2022 04:49:57 PM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.1, max_depth=20, colsample_bytree=0.7, n_estimators =1000 with the adjusted R2 score of 0.6881611835582412
INFO 06/06/2022 04:50:17 PM TRAINING: Best xgb classifier trained
INFO 06/06/2022 04:50:17 PM TRAINING: Search for best xgb classifier model ended
INFO 06/06/2022 04:50:17 PM TRAINING: The best model is xgb classifier
INFO 06/06/2022 04:50:17 PM TRAINING: Search for best model started
INFO 06/06/2022 04:50:17 PM TRAINING: Search for best model started
INFO 06/06/2022 04:50:17 PM TRAINING: Search for best logistic regressor model started
INFO 06/06/2022 04:50:17 PM TRAINING: Search for best logistic regressor model started
INFO 06/06/2022 04:50:17 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/06/2022 04:50:17 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/06/2022 04:50:24 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.01, penalty=l2,solver=newton-cg  with the f1 score of 0.7879674965219553
INFO 06/06/2022 04:50:24 PM TRAINING: The optimum parameters of Logistic Regressor are C=0.01, penalty=l2,solver=newton-cg  with the f1 score of 0.7879674965219553
INFO 06/06/2022 04:50:24 PM TRAINING: Best Logistic  Regressor trained
INFO 06/06/2022 04:50:24 PM TRAINING: Best Logistic  Regressor trained
INFO 06/06/2022 04:50:24 PM TRAINING: Search for best ridge model ended
INFO 06/06/2022 04:50:24 PM TRAINING: Search for best ridge model ended
INFO 06/06/2022 04:50:24 PM TRAINING: Search for best svc model started
INFO 06/06/2022 04:50:24 PM TRAINING: Search for best svc model started
INFO 06/06/2022 04:50:24 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/06/2022 04:50:24 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/06/2022 05:10:19 PM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=scale, C=3, degree =2 with the f1 score of 0.801033074068013
INFO 06/06/2022 05:10:19 PM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=scale, C=3, degree =2 with the f1 score of 0.801033074068013
INFO 06/06/2022 05:10:24 PM TRAINING: Best SVC trained
INFO 06/06/2022 05:10:24 PM TRAINING: Best SVC trained
INFO 06/06/2022 05:10:27 PM TRAINING: Search for best svc model ended
INFO 06/06/2022 05:10:27 PM TRAINING: Search for best svc model ended
INFO 06/06/2022 05:10:27 PM TRAINING: Search for best random forest classifier model started
INFO 06/06/2022 05:10:27 PM TRAINING: Search for best random forest classifier model started
INFO 06/06/2022 05:10:27 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/06/2022 05:10:27 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/06/2022 06:04:45 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=300, criterion=entropy, min_samples_split=2, max_features =log2, ccp_alpha=0.0 with the adjusted R2 score of 0.816782192750478
INFO 06/06/2022 06:04:45 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=300, criterion=entropy, min_samples_split=2, max_features =log2, ccp_alpha=0.0 with the adjusted R2 score of 0.816782192750478
INFO 06/06/2022 06:04:50 PM TRAINING: Best random forest classifier trained
INFO 06/06/2022 06:04:50 PM TRAINING: Best random forest classifier trained
INFO 06/06/2022 06:04:50 PM TRAINING: Search for best random forest classifier model ended
INFO 06/06/2022 06:04:50 PM TRAINING: Search for best random forest classifier model ended
INFO 06/06/2022 06:04:50 PM TRAINING: Search for best xgb classifier model started
INFO 06/06/2022 06:04:50 PM TRAINING: Search for best xgb classifier model started
INFO 06/06/2022 06:04:50 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/06/2022 06:04:50 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/06/2022 08:50:12 PM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=20, colsample_bytree=0.5, n_estimators =3000 with the adjusted R2 score of 0.8190509888912064
INFO 06/06/2022 08:50:12 PM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=20, colsample_bytree=0.5, n_estimators =3000 with the adjusted R2 score of 0.8190509888912064
INFO 06/06/2022 08:51:24 PM TRAINING: Best xgb classifier trained
INFO 06/06/2022 08:51:24 PM TRAINING: Best xgb classifier trained
INFO 06/06/2022 08:51:24 PM TRAINING: Search for best xgb classifier model ended
INFO 06/06/2022 08:51:24 PM TRAINING: Search for best xgb classifier model ended
INFO 06/06/2022 08:51:24 PM TRAINING: The best model is xgb classifier
INFO 06/06/2022 08:51:24 PM TRAINING: The best model is xgb classifier
INFO 06/06/2022 08:51:25 PM TRAINING: Search for best model started
INFO 06/06/2022 08:51:25 PM TRAINING: Search for best model started
INFO 06/06/2022 08:51:25 PM TRAINING: Search for best model started
INFO 06/06/2022 08:51:25 PM TRAINING: Search for best logistic regressor model started
INFO 06/06/2022 08:51:25 PM TRAINING: Search for best logistic regressor model started
INFO 06/06/2022 08:51:25 PM TRAINING: Search for best logistic regressor model started
INFO 06/06/2022 08:51:25 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/06/2022 08:51:25 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/06/2022 08:51:25 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/06/2022 08:51:37 PM TRAINING: The optimum parameters of Logistic Regressor are C=3, penalty=l2,solver=newton-cg  with the f1 score of 0.7499971280277078
INFO 06/06/2022 08:51:37 PM TRAINING: The optimum parameters of Logistic Regressor are C=3, penalty=l2,solver=newton-cg  with the f1 score of 0.7499971280277078
INFO 06/06/2022 08:51:37 PM TRAINING: The optimum parameters of Logistic Regressor are C=3, penalty=l2,solver=newton-cg  with the f1 score of 0.7499971280277078
INFO 06/06/2022 08:51:37 PM TRAINING: Best Logistic  Regressor trained
INFO 06/06/2022 08:51:37 PM TRAINING: Best Logistic  Regressor trained
INFO 06/06/2022 08:51:37 PM TRAINING: Best Logistic  Regressor trained
INFO 06/06/2022 08:51:37 PM TRAINING: Search for best ridge model ended
INFO 06/06/2022 08:51:37 PM TRAINING: Search for best ridge model ended
INFO 06/06/2022 08:51:37 PM TRAINING: Search for best ridge model ended
INFO 06/06/2022 08:51:37 PM TRAINING: Search for best svc model started
INFO 06/06/2022 08:51:37 PM TRAINING: Search for best svc model started
INFO 06/06/2022 08:51:37 PM TRAINING: Search for best svc model started
INFO 06/06/2022 08:51:37 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/06/2022 08:51:37 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/06/2022 08:51:37 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/06/2022 09:10:12 PM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=auto, C=10, degree =2 with the f1 score of 0.7643327386282025
INFO 06/06/2022 09:10:12 PM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=auto, C=10, degree =2 with the f1 score of 0.7643327386282025
INFO 06/06/2022 09:10:12 PM TRAINING: The optimum parameters of SVC are kernel=rbf, gamma=auto, C=10, degree =2 with the f1 score of 0.7643327386282025
INFO 06/06/2022 09:10:14 PM TRAINING: Best SVC trained
INFO 06/06/2022 09:10:14 PM TRAINING: Best SVC trained
INFO 06/06/2022 09:10:14 PM TRAINING: Best SVC trained
INFO 06/06/2022 09:10:16 PM TRAINING: Search for best svc model ended
INFO 06/06/2022 09:10:16 PM TRAINING: Search for best svc model ended
INFO 06/06/2022 09:10:16 PM TRAINING: Search for best svc model ended
INFO 06/06/2022 09:10:16 PM TRAINING: Search for best random forest classifier model started
INFO 06/06/2022 09:10:16 PM TRAINING: Search for best random forest classifier model started
INFO 06/06/2022 09:10:16 PM TRAINING: Search for best random forest classifier model started
INFO 06/06/2022 09:10:16 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/06/2022 09:10:16 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/06/2022 09:10:16 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['n_estimators', 'criterion', 'min_samples_split', 'max_features', 'ccp_alpha'])) of random forest classifier 
INFO 06/06/2022 09:49:15 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=300, criterion=gini, min_samples_split=3, max_features =auto, ccp_alpha=0.0 with the adjusted R2 score of 0.7888743164753937
INFO 06/06/2022 09:49:15 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=300, criterion=gini, min_samples_split=3, max_features =auto, ccp_alpha=0.0 with the adjusted R2 score of 0.7888743164753937
INFO 06/06/2022 09:49:15 PM TRAINING: The optimum parameters of random forrest classifier are n_estimators=300, criterion=gini, min_samples_split=3, max_features =auto, ccp_alpha=0.0 with the adjusted R2 score of 0.7888743164753937
INFO 06/06/2022 09:49:17 PM TRAINING: Best random forest classifier trained
INFO 06/06/2022 09:49:17 PM TRAINING: Best random forest classifier trained
INFO 06/06/2022 09:49:17 PM TRAINING: Best random forest classifier trained
INFO 06/06/2022 09:49:17 PM TRAINING: Search for best random forest classifier model ended
INFO 06/06/2022 09:49:17 PM TRAINING: Search for best random forest classifier model ended
INFO 06/06/2022 09:49:17 PM TRAINING: Search for best random forest classifier model ended
INFO 06/06/2022 09:49:17 PM TRAINING: Search for best xgb classifier model started
INFO 06/06/2022 09:49:17 PM TRAINING: Search for best xgb classifier model started
INFO 06/06/2022 09:49:17 PM TRAINING: Search for best xgb classifier model started
INFO 06/06/2022 09:49:17 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/06/2022 09:49:17 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/06/2022 09:49:17 PM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['learning_rate', 'colsample_bytree', 'max_depth', 'n_estimators', 'verbosity'])) of xgb classifier
INFO 06/06/2022 11:31:47 PM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=15, colsample_bytree=0.8, n_estimators =3000 with the adjusted R2 score of 0.8007700396809346
INFO 06/06/2022 11:31:47 PM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=15, colsample_bytree=0.8, n_estimators =3000 with the adjusted R2 score of 0.8007700396809346
INFO 06/06/2022 11:31:47 PM TRAINING: The optimum parameters of xgb-classifier are learning_rate=0.03, max_depth=15, colsample_bytree=0.8, n_estimators =3000 with the adjusted R2 score of 0.8007700396809346
INFO 06/06/2022 11:32:36 PM TRAINING: Best xgb classifier trained
INFO 06/06/2022 11:32:36 PM TRAINING: Best xgb classifier trained
INFO 06/06/2022 11:32:36 PM TRAINING: Best xgb classifier trained
INFO 06/06/2022 11:32:37 PM TRAINING: Search for best xgb classifier model ended
INFO 06/06/2022 11:32:37 PM TRAINING: Search for best xgb classifier model ended
INFO 06/06/2022 11:32:37 PM TRAINING: Search for best xgb classifier model ended
INFO 06/06/2022 11:32:37 PM TRAINING: The best model is xgb classifier
INFO 06/06/2022 11:32:37 PM TRAINING: The best model is xgb classifier
INFO 06/06/2022 11:32:37 PM TRAINING: The best model is xgb classifier
INFO 06/08/2022 08:56:14 AM TRAINING: Search for best model started
INFO 06/08/2022 08:56:14 AM TRAINING: Search for best logistic regressor model started
INFO 06/08/2022 08:56:14 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/08/2022 08:56:24 AM TRAINING: The optimum parameters of Logistic Regressor are C=0.01, penalty=l2,solver=newton-cg  with the f1 score of 0.6048296160768858
INFO 06/08/2022 08:56:24 AM TRAINING: Best Logistic  Regressor trained
INFO 06/08/2022 08:56:24 AM TRAINING: Search for best ridge model ended
INFO 06/08/2022 08:56:24 AM TRAINING: Search for best svc model started
INFO 06/08/2022 08:56:24 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
INFO 06/08/2022 09:30:59 AM TRAINING: Search for best model started
INFO 06/08/2022 09:30:59 AM TRAINING: Search for best logistic regressor model started
INFO 06/08/2022 09:30:59 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'penalty', 'solver']))  of Logistic Regressor
INFO 06/08/2022 09:31:07 AM TRAINING: The optimum parameters of Logistic Regressor are C=0.01, penalty=l2,solver=newton-cg  with the f1 score of 0.7922389371433322
INFO 06/08/2022 09:31:07 AM TRAINING: Best Logistic  Regressor trained
INFO 06/08/2022 09:31:07 AM TRAINING: Search for best ridge model ended
INFO 06/08/2022 09:31:07 AM TRAINING: Search for best svc model started
INFO 06/08/2022 09:31:07 AM TRAINING: Using GridSearchCV to obtain the optimum parameters(dict_keys(['C', 'kernel', 'degree', 'gamma'])) of SVC
ERROR 06/08/2022 09:32:07 AM TRAINING: There was a problem while fitting SVC: 
ERROR 06/08/2022 09:32:07 AM TRAINING: There was a problem while obtaining best model : 
